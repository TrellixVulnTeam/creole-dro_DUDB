//local transformer_model = std.extVar("BERT_PRETRAINED")
local transformer_dim = 768;

//local transformer_model = "xlm-roberta-large";
//local transformer_dim = 1024;

local max_len = 128;

{
    "random_seed": 8446,
    "numpy_seed": 8446,
    "pytorch_seed": 8446,
    "dataset_reader": {
        // this does not currently seem to work not sure why
        "type": "machamp_universal_reader",
        //"type": "sequence_tagging",
        //does not help
        "target_max_tokens": max_len,
        "source_max_tokens": max_len,
        "do_lowercase": false,

        "token_indexers": {
            "tokens": {
                "type": "pretrained_transformer_mixmatched",
                "max_length": max_len,
                "model_name": std.extVar("BERT_PRETRAINED"),
            }
        },
        "tokenizer": {
            "type": "pretrained_transformer",
            "add_special_tokens": false,
            "model_name": std.extVar("BERT_PRETRAINED"),
        },
        "target_token_indexers": {
            "tokens": {
                "namespace": "target_words"
            }
        },
        "target_tokenizer":{
             "type": "bert_basic_tokenizer"
        }
    },
    "vocabulary": {
        "max_vocab_size": {"target_words": 50000},
        "min_count": {
            "source_words": 1,
            "target_words": 1
        }
    },
    "model": {
        "type": "machamp_model",
        "dataset_embeds_dim": 0,
        "decoders": {
            "default": {
                "input_dim": transformer_dim,
                "loss_weight": 1,
                "order": 1
            },
            "seq_bio": {
                "type": "machamp_crf_decoder",
                "metric": "span_f1"
            }
        },
        "default_max_sents": 0,
        "dropout": 0.3,
        "encoder": {
            "type": "cls_pooler",
            "cls_is_last_token": false,
            "embedding_dim": transformer_dim
        },
        "text_field_embedder": {
            "type": "basic",
            "token_embedders": {
                "tokens": {
                    "type": "machamp_pretrained_transformer_mismatched",
                    "layers_to_use": [-1],
                    "max_length": max_len,
                    "model_name": std.extVar("BERT_PRETRAINED"),
                    "train_parameters": true
                }
            }
        }
    },
    "data_loader": {
        "batch_sampler": {
            "type": "dataset_buckets",
            "max_tokens": 1024,
            "batch_size": std.parseInt(std.extVar('BATCH_SIZE')),
            "sampling_smoothing": 1.0, //1.0 == original size
            "sorting_keys": [
                "tokens"
            ]
        }
    },
    "trainer": {
        "checkpointer": {
            "num_serialized_models_to_keep": 1
        },
        "use_amp": false, // could save some memory on gpu
        "grad_norm": 1,
        "learning_rate_scheduler": {
            "type": "slanted_triangular",
            "cut_frac": 0.2,
            "decay_factor": 0.38,
            "discriminative_fine_tuning": true,
            "gradual_unfreezing": true
        },
        "num_epochs": std.parseInt(std.extVar('EPOCHS')),
        "optimizer": {
            "type": "huggingface_adamw",
            "betas": [0.9, 0.99],
            "correct_bias": false,
            "lr": 0.0001,
            "parameter_groups": [
                [
                    [
                        "^_text_field_embedder.*"
                    ],
                    {}
                ],
                [
                    [
                        "^decoders.*",
                        "dataset_embedder.*"
                    ],
                    {}
                ]
            ],
            "weight_decay": 0.01
        },
        //"patience": 5, // disabled, because slanted_triangular changes the lr dynamically
        "validation_metric": "+.run/.sum"
    },
    "datasets_for_vocab_creation": [
        "train",
        "validation"//TODO can this be removed now that we add padding/unknown?
    ]
}

